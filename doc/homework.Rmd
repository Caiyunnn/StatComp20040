---
title: "Homework"
author: "SC20040"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

#  <font color=Teal>2020/09/22</font>

## Question
The 1st example should contain texts and at least one figure.
The 2nd example should contains texts and at least one table. 
The 3rd example should contain at least a couple of LaTeX formulas.

## Answer
例一：利用R的内置数据集sleep，通过用两种颜色和形状的散点图和拟合的平滑的线条来分别表现两种药物对于十位病人的睡眠改善情况，横坐标patientsID的取值范围为1-10，分别对应十位病人，纵坐标为increase in hours of sleep，表示服用药物后的睡时长增减情况，蓝色表示
药物1的作用效果，红色代表药物2的作用效果，绿色水平辅助线帮助总体上辨别出病人睡眠状况恶化的样本点。
```{r}
library(car)
attach(sleep)                                                        #加载数据                                                                                      
data=sleep                                                
data$ID=as.numeric(sleep$ID)
scatterplot(extra~ID|group,data=data, main= "two drugs'effect",      #创建散点图
            legend = list(coords="topleft"),boxplot='y',             #设置图像的参数
            xlab = "patientsID",regLine=FALSE,                       
            ylab = "increase in hours of sleep")
abline(h=0, col = "green",lwd=2,lty=1)                               #增加直线：Y=0
detach(sleep)
```
例二使用了R内部的数据集women,先进行身高关于体重的回归，然后利用kable将其方差分析的结果以表格的形式展现出来。
```{r}
data(women)
attach(women)                                             #加载数据集
library(knitr)                                            #加载包
lm.D1 = lm(height~weight,data = women)                    #回归
knitr::kable(anova(lm.D1),caption = "方差分析表")         #生成表格
detach(women)
```
例三为欧拉公式
$$e^{ix}=cosx+isinx$$

#  <font color=Teal>2020/09/29</font>

## Question 3.3

The Pareto(a, b) distribution has cdf
$$F(x)=1−(\frac{b}{x})^{a},  x \ge b>0, a>0.$$
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto(2, 2) distribution. Graph the density histogram of the sample with the Pareto(2, 2)
density superimposed for comparison.

## Answer 3.3

```{r}
n <- 1000
u <- runif(n)               
x <- 2/(1-u)^{1/2}  #the inverse transform method: F(x) = 1-(2/x)^2, x>=2
hist(x, prob = TRUE, main = expression(f(x)==frac(8,x^3))) #Graph the density histogram of the sample
y <- seq(2, 1000, .01)
lines(y, 8/y^3, col="red")
```



## Question 3.9
The rescaled Epanechnikov kernel is a symmetric density function $$f_{e}(x)=\frac{3}{4}(1-x^{2}), \left\vert x \right\vert \le 1.$$
Devroye and Gyorfi  give the following algorithm for simulation from this distribution. Generate iid $U_{1},U_{2},U_{3} \sim U(−1, 1)$. If $\left\vert U_{3} \right\vert \ge  \left\vert U_{2} \right\vert$ and $\left\vert U_{3} \right\vert \ge \left\vert U_{1} \right\vert$, deliver $U_{2}$; otherwise deliver $U_{3}$. Write a function to generate random variates from $f_{e}$, and construct the histogram density
estimate of a large simulated random sample.

## Answer 3.9

```{r}
set.seed(123)
n <- 1e5;k<-0;y <- numeric(n)
U1 <- runif(n,-1,1)
U2 <- runif(n,-1,1)
U3 <- runif(n,-1,1)                #U1,U2,U3~U(0,1)
while (k < n) {
  k <- k + 1
  if (abs(U1[k])<=abs(U3[k]) && abs(U2[k])<=abs(U3[k])) {
  U3[k] <- U2[k]
  }
} #when |U1|<=|U3|and |U2|<=|U3|，replace U3 with U2
hist(U3,  prob = TRUE,main = expression(f(x)==frac(3,4)*(1-x^2)),ylim = c(0,0.8))
# estimate of a large simulated random sample
y <- seq(-1,1, .01)
lines(y, (3/4)*(1-y^2),col="red")

```



## Question 3.10
Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_{e} (3.10)$.

## Answer 3.10

### proof: 

according to exercise 3.9:

$\because U_{1},U_{2},U_{3} \sim U(−1, 1)$

$\therefore f(u_1,u_2,u_3)=\frac{1}{8}$

\begin{align*}
f_e(x)&=\iint_{\lvert U_{3} \rvert \ge \lvert U_{2} \rvert, \lvert U_{3} \rvert \ge \lvert U_{1} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{3} \\
&+ \iint_{\lvert U_{3} \rvert < \lvert U_{2} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{2}\\
&+\iint_{\lvert U_{3} \rvert < \lvert U_{1} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{2}\\
&-\iint_{\lvert U_{3} \rvert < \lvert U_{2} \rvert, \lvert U_{3} \rvert < \lvert U_{1} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{2}
\end{align*}


\begin{align*}
&\iint_{\lvert U_{3} \rvert \ge \lvert U_{2} \rvert, \lvert U_{3} \rvert \ge \lvert U_{1} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{3} \\
=&2\int_{0}^{\lvert x \rvert} 2\int_{\max\{\lvert x \rvert,\lvert u_{1} \rvert\}}^{1} \frac{1}{8}\, du_{3}du_{1}\\
=&\int_{0}^{\lvert x \rvert} \frac{1}{2}(1-\frac{1}{2}(\lvert x \rvert+\lvert u_{1}\rvert+\lvert \lvert x \rvert-\lvert u_{1} \rvert \rvert)\, du_{1}\\
=&\int_{0_{\lvert u_{1} \rvert < \lvert x \rvert}}^{\lvert x \rvert} \frac{1}{2}-\frac{1}{2}\lvert x \rvert \, du_{1}+\int_{0_{\lvert u_{1} \rvert \ge \lvert x \rvert}}^{\lvert x \rvert} \frac{1}{2}-\frac{1}{2}\lvert u_{1} \rvert \, du_{1} \\
=&\frac{1}{4}-\frac{x^2}{4}
\end{align*}

obviously:
\begin{align*}
&\iint_{\lvert U_{3} \rvert < \lvert U_{2} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{2}\\
=&\iint_{\lvert U_{3} \rvert < \lvert U_{1} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{2}\\
=&\int_{-1}^{1} \int_{\lvert x \rvert}^{1}2*\frac{1}{8}du_{1}du_{2}\\
=&\frac{1-\lvert x \rvert}{2}
\end{align*}

\begin{align*}
&\iint_{\lvert U_{3} \rvert < \lvert U_{2} \rvert, \lvert U_{3} \rvert < \lvert U_{1} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{2} \\
=&\int_{\lvert x \rvert}^{1} \int_{\lvert x \rvert}^{1} 2*2*\frac{1}{8}du_{1}du_{2}\\
=&\frac{x^2}{2}+\frac{1}{2}-\frac{\lvert x \rvert}{2}
\end{align*}

\begin{align*}
\therefore
f_e(x)&=\iint_{\lvert U_{3} \rvert \ge \lvert U_{2} \rvert, \lvert U_{3} \rvert \ge \lvert U_{1} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{3} \\
&+ \iint_{\lvert U_{3} \rvert < \lvert U_{2} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{2}\\
&+\iint_{\lvert U_{3} \rvert < \lvert U_{1} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{2}\\
&-\iint_{\lvert U_{3} \rvert < \lvert U_{2} \rvert, \lvert U_{3} \rvert < \lvert U_{1} \rvert} f(u_{1},u_{2},u_{3})\, du_{1}du_{2}\\
&=\frac{3}{4}(1-x^{2})
\end{align*}




## Question 3.13
It can be shown Exponential-Gamma mixture has a Pareto distribution with cdf
$$F(y)=1-(\frac{\beta}{\beta+y})^{r}, y \ge 0.$$
Generate 1000 random observations from the mixture with $r=4$ and $β = 2$. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density
curve.



## Answer 3.13


```{r}
set.seed(123)
n <- 1e5; r <- 4; beta <- 2
lambda <- rgamma(n, r, beta)
x <- rexp(n, lambda) # the length of lambda = n
hist(x, prob = TRUE, main = expression(f(x)==frac(64,(2+y)^5)),ylim = c(0,0.5))
# the density histogram of the sample 
y <- seq(0, 1000, .01)
lines(y, 64/(2+y)^5, col="red")

```


#  <font color=Teal>2020/10/13</font>

## Question 5.1
Compute a Monte Carlo estimate of
$$\int_0^{\frac{\pi}{3}} \sin tdt$$

## Answer 5.1

```{r}
library(knitr)
n <- 1e5
x <- runif(n, min=0, max=pi/3)         #generate random x~u(0,pi/3)
theta.hat <- mean(sin(x))*pi/3         #Monte Carlo estimate

 #creat a table to compare estimation with exact value
knitr::kable(data.frame(c(theta.hat,-cos(pi/3) + cos(0)),row.names=c('estimate','exact')),row.names = TRUE,col.names = "value", caption = "Comparison of estimation and exact value", align = "c")

```



## Question 5.7
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate θ by the
antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic
variate. Compare the result with the theoretical value from Exercise 5.6.

## Answer 5.7

```{r}
library(knitr)
library(scales)
#generate random x~u(0,1)
n <- 1e5
x <- runif(n, min=0, max=1)

# simple Monte Carlo method
theta1.hat <- mean(exp(x))
theoreticalValue <- exp(1)-exp(0)

#antithetic variate approach
y <- x[1:(n/2)]
theta2.hat <- mean(exp(y)+exp(1-y))/2

#estimate two kinds methods' variation
var1 <- var(exp(x))/n
var2 <- var(exp(y)+exp(1-y))/(2*n)
reduction <- percent((var1-var2)/var1)

#create a table to campare
z<-data.frame(c(theta1.hat,theoreticalValue,var1),c(theta2.hat,theoreticalValue,var2),c("\\","\\",reduction),row.names = c("theta.hat","theta","variance"))
colnames(z)= c("simpleMC","antithetic","redutionPercent")
knitr::kable(z,row.names = TRUE, caption = "Comparison of two estimations",align = 'c')

```



## Question 5.11
If $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are unbiased estimators of $\theta$,and $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are antithetic, we derived that $c^{*} =1/2$ is the optimal constant that minimizes the variance of $\hat{\theta}_{c}=c\hat{\theta}_{1} +(1 − c)\hat{\theta}_{2}$.Derive $c^{*}$ for the general case. That is, if $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are any two unbiased estimators of $\theta$, find the value $c^{*}$ that minimizes the variance of the estimator $\hat{\theta}_{c}=c\hat{\theta}_{2} +(1 − c)\hat{\theta}_{2}$ in equation. 


## Answer 5.11

### proof: 


\begin{align*}
\because
&Var(\hat{\theta}_c)\\
=&c^2Var(\hat{\theta}_1)+(1-c)^2Var(\hat{\theta}_2)+2c(1-c)cov(\hat{\theta}_1,\hat{\theta}_2) \\
=&(Var(\hat{\theta}_1)+Var(\hat{\theta}_2)-2cov(\hat{\theta}_1,\hat{\theta}_2))c^2-2(Var(\hat{\theta}_2)-cov(\hat{\theta}_1,\hat{\theta}_2))c+Var(\hat{\theta}_2)\\
=&Var(\hat{\theta}_1-\hat{\theta}_2)c^2+2cov(\hat{\theta}_1-\hat{\theta}_2,\hat{\theta}_2)c+Var(\hat{\theta}_2) \\
=&Var(\hat{\theta}_1-\hat{\theta}_2)(c+\frac{cov(\hat{\theta}_{1}-\hat{\theta}_{2},\hat{\theta}_2)}{Var(\hat{\theta}_1-\hat{\theta}_2)})^{2}-\frac{cov^{2}(\hat{\theta}_1-\hat{\theta}_2,\hat{\theta}_2)}{Var(\hat{\theta}_1-\hat{\theta}_2)}+Var(\hat{\theta}_2)
\end{align*}

$\therefore$,$c^{*}=-\frac{Cov(\hat{\theta}_{1}-\hat{\theta}_{2},\hat{\theta}_{2})}{Var(\hat{\theta}_{1}-\hat{\theta}_{2})}$

#  <font color=Teal>2020/10/20</font>

## Question 5.1
Find two importance functions $f1$ and $f2$ that are supported on $(1, ∞)$ and
are ‘close’ to
$$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},x>1$$
Which of your two importance functions should produce the smaller variance
in estimating
$$\int_1^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx$$
## Answer 5.1
{\textb}根据泰勒展开公式，在x=0处展开，$e^{-\frac{x^2}{2}}=1-x^2+3x^4+o(x^4)$  
因此我们取最简形式，并且保证$f(x)在(1，\infty)积分为1$  
$f1=e^{1-x},f2=\frac{1}{x^2}$

```{r}
library(knitr)
n <- 1e5
x <- runif(n)
y <- 1-log(1-x)     

#生成服从密度函数为f2的随机数
u <- runif(n)
v <- 1/(1-u)
g <- function(x){(x^2 *exp(-x^2/2))/sqrt(2*pi)}
f1 <- function(x){exp(1-x)}  #设置第一个影响函数
f2 <- function(x){1/x^2}  #设置第二个影响函数
theta1 <- mean(g(y)/f1(y))
theta2 <- mean(g(v)/f2(v))
var1 <- var(g(y)/f1(y))/length(y)
var2 <- var(g(v)/f2(v))/length(u)
z <- seq(1,4,0.01)

#生成图像比较
plot(z, g(z)/f1(z), type = "l", ylab = "",ylim = c(0,3.2), lwd = 2, lty = 2,col=2,main='(ratio of g and f)')
lines(z,g(z)/f2(z), lty = 3, lwd = 2,col=3)
legend("topright",legend=c(expression(f[1](x)==e^{-x}),expression(f[2](x)==4/((1+x^2)*pi))),lty = 2:3, lwd = 2, inset = 0.02,col=2:3)


 #生成表格比较俩种结果
knitr::kable(data.frame(c(theta1,var1),c(theta2,var2),row.names=c('thetahat','thetahat.var')),row.names = TRUE,col.names = c("f1","f2"), caption = "Comparison of two estimations", align = "c")

```



## Question 5.15
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

## Answer 5.15
divide the interval (0,1) into five subintervals, (j/5,(j + 1)/5), j = 0, 1,..., 4.

```{r}
n <- 1e4
r <- n/5         #replicates per stratum
N <- 50          #number of times to repeat the estimation
T2 <- numeric(5)
est <- matrix(0, N, 2)
g1<-function(x){(1/(1+x^2))*(x>0)*(x<1)}

for (j in 1:N){
  y <- rexp(n,rate = 1)
  est[j, 1] <- mean(g1(y))
for (i in 1:5){
    u <- runif(r)
    v <- -log(exp(-(i-1)/5)*(1-u)+exp(-i/5)*u)
    g <- function(x){((exp(-(i-1)/5)-exp(-i/5))/(1+x^2))*(x>(i-1)/5)*(x<i/5)}
    T2[i] <- mean(g(v))
}
  est[j, 2] <- sum(T2)
}
apply(est,2,mean)

```



## Question 6.4
Suppose that $X_1,...,X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for
the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate
of the confidence level.

## Answer 6.4
```{r}
alpha <- .05
UCL <- numeric(1000)
LCL <- numeric(1000)

#生成计算置信区间的函数
CI <- function(n,alpha){
 x <- rlnorm(n, meanlog = 0,sdlog = 1) 
 y <- log(x)
 UCL <-mean(y)+ sd(y)*qt(1-alpha/2, df = n-1)/sqrt(n)
 LCL <-mean(y)+ sd(y)*qt(alpha/2, df = n-1)/sqrt(n)
 return(c(LCL,UCL))
}

k <- replicate(1000,CI(20,0.05))
sum((k[1,]<0)*(k[2,]>0))/1000



```


## Question 6.5
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval
covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment
to estimate the coverage probability of the t-interval for random samples of
$\chi^2$(2) data with sample size n = 20. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)

## Answer 6.5
```{r}
alpha <- .05
UCL <- numeric(1000)
LCL <- numeric(1000)

#生成计算置信区间的函数
CI <- function(n,alpha){
 x <- rchisq(n,2) 
 UCL <-mean(x)+ sd(x)*qt(1-alpha/2, df = n-1)/sqrt(n)
 LCL <-mean(x)+ sd(x)*qt(alpha/2, df = n-1)/sqrt(n)
 return(c(LCL,UCL))
}

#x服从自由度为2的卡方分布，E(x)=2
k <- replicate(1000,CI(20,0.05))
sum((k[1,]<2)*(k[2,]>2))/1000

```

#  <font color=Teal>2020/10/27</font>
## Question 6.7
Estimate the power of the skewness test of normality against symmetric
Beta$(\alpha, \alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(ν)$?



## Answer 6.7
{\textb}We estimate by simulation the power of the skewness test of beta distribution against a contaminated beta alternative.The contaminated beta distribution is denoted by $(1-t)\beta(2,2)+t\beta(10,10)$.


```{r}
#pure distribution
alpha <-0.05
#creat a function to compute the sample skewness statistic.
sk <- function(x) {
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return(m3/m2^1.5)
}
n <- 40
cv <- qnorm(1-alpha/2, 0,sqrt(6*(n-2)/((n+1)*(n+3))))
m <- 4000
t <- c(seq(0, .15, .01), seq(.15, 1, .05))
N <- length(t)
test1 <- test2 <- numeric(m)
a <- b <- numeric(n)
POWER <- matrix(0,N,2)
for (i in 1:N) {
  p <- t[i]
  for (j in 1:m) {
    a <- sample(c(2, 10), replace = TRUE, size = n, prob = c(1-p,p))
    x <- rbeta(n,a,a)
    test1[j] <- as.integer(abs(sk(x)) >= cv)     #mixed beta distribution
    b <- sample(c(2, 5), replace = TRUE, size = n, prob = c(1-p,p))
    y <- rt(n,df=b)
    test2[j] <- as.integer(abs(sk(y)) >= cv)    #mixed t distribution
  }
  POWER[i,1] <- mean(test1)
  POWER[i,2] <- mean(test2)
}             
# graph to compare
plot(t, POWER[,1], xlab = bquote(t), ylim = c(0,1),col=2,type = "b")
lines(t,POWER[,2],xlab = bquote(t),ylim = c(0,1),col=3,type = "b")
legend("topright", legend=c("beta ditribution","t distribution"), col=c(2,3)) 

```







## Question 6.8
Refer to Example 6.16. Repeat the simulation, but also compute the F test
of equal variance, at significance level $\hat\alpha$ = 0.055. Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall
that the F test is not applicable for non-normal distributions.)


## Answer 6.8
{\textb}


```{r}
k <-matrix(0,2,3)
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(as.integer(max(c(outx, outy)) > 5))
}

power <- function(n,alpha){
  x <- rnorm(n, 0, sigma1)
  y <- rnorm(n, 0, sigma2)
  pow1 <- count5test(x, y)
  pow2 <- as.integer(var.test(x,y,ratio = 1,conf.level = 1-alpha)$p.value < alpha)
  return(c(pow1,pow2))
}

# generate samples under H1 to estimate power
sigma1 <- 1
sigma2 <- 2
alpha  <- 0.055
m <- 1000
n <- c(20,100,1000)
for (i in 1:length(n)) {
  k[,i] <- apply(replicate(m,power(n[i],alpha)),MARGIN = 1,mean)
}
result <- as.data.frame(k,row.names = c("count five test","F test"))
colnames(result) <- c("small","medium","large")
print(result)
```


## Question 6.C
Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X and Y$ are iid, the multivariate
population skewness $\beta_{1,d}$ is defined by Mardia as
$\beta_{1,d}=E[X-\mu]^T\sum^{-1}(Y-\mu)]^3$  
Under normality, β1,d = 0. The multivariate skewness statistic is  
$b_{1,d}=\frac{1}{n^2}\sum_{i,j=1}^n((X-\mu)^T\hat\sum^{-1}(X_j-\bar X))^3$,
where $\hat\sum$ is the maximum likelihood estimator of covariance. Large values of
$b_{1,d}$ are significant. The asymptotic distribution of $nb_{1,d}/6$ is chisquared with  $d(d + 1)(d + 2)/6$ degrees of freedom.

## Answer 6.C
```{r}
#对于n*d维矩阵x，(x1,x2....xn),每个样本含有d个变量,s为协方差矩阵

b_1d <- function(x,d){
  xbar <- apply(x, margin=1, mean)
   y <- x-xbar
   s <- solve(cov(x))
   b_1d <- (t(y)%*%s%*%y)^3/n^2
}

```


## Question discussion
If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

## Answer

{\textb}:1.What is the corresponding hypothesis test problem?

$H_0:power_1 = power_2 \qquad H_1:power_1 \ne power_2$

2.What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test?

Beacause we obtain the powers for two methods under a particular simulation setting with 10,000 experiments,which is based on the data of homologous pairing, we can use Z-test, paired-t test and McNemar test except two-sample t-test.Under the premise of a large sample,we'd better choose Z-test and McNemar test.Under the premise of a small sample,paired-t test is more suitable.

3.What information is needed to test your hypothesis?
the powers for two methods under a particular simulation setting with 10,000 experiments

#  <font color=Teal>2020/11/03</font>
## Question 9.4
Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

## Answer 9.4
{\textb}

```{r}
maxout <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
return(max(c(outx, outy)))
}
   
```



## Question 7.5




## Answer 7.5
```{r}
set.seed(1)
m<-1e3 #number of cycles
B<-1e3 #The number of bootstrap replicates
library(boot)#for boot and boot.ci function
#generates 4 different types of equi-tailed two-sided nonparametric confidence intervals
for(i in 1:m){
  boot.obj <- boot(data=aircondit,statistic=function(x,i){mean(x[i,])}, R=B )
  CI <- boot.ci(boot.obj,conf=0.95,type=c("norm","basic","perc","bca"))

}
print(CI)

```

#  <font color=Teal>2020/11/10</font>

## Question 8.3
The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

## Answer 8.3
{\textb}

```{r}
vartest <- function(x, y) {
x0 <- x - mean(x)
y0 <- y - mean(y)
outx <- sum(x0 > max(y0)) + sum(x0 < min(y0))
outy <- sum(y0 > max(x0)) + sum(y0 < min(x0))
return(max(c(outx, outy)))
}

set.seed(1234)
R <- 999
n1 <- 20
n2 <- 40
n <- n1+n2  
m <- 1000
p <- numeric(R)
p.hat <- numeric(m)
test <- replicate(m,expr = {
x <- rnorm(n1,0,1)
y <- rnorm(n2,0,1)
z <- c(x, y)
temp <- vartest(x,y)
for (i in 1:R) {
ind <- sample(n, size = n1, replace = FALSE)
x1 <- z[ind]
y1 <- z[-ind] 
p[i] <-vartest(x1, y1)
}
p.hat <- (1+sum(as.integer(p>temp)))/1000
})
print(round(mean(test),6))

   
```
analysis: Pvalue平均值为0.417，大于0.05，因此没有充分理由拒绝原假设，所以两样本为同分布。

## Question 2
Design experiments for evaluating the performance of the NN,energy, and ball methods in various situations.
1.Unequal variances and equal expectations
2.Unequal variances and unequal expectations
3.Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)
4.Unbalanced samples (say, 1 case versus 10 controls)
Note: The parameters should be chosen such that the power are distinguishable (say, range from 0.3 to 0.8).

analysis:
1.For the first situation, we set sample with $X \sim N(0,1),Y \sim N(0,4)$
2.For the second situation, we set sample with $X \sim N(0,1),Y \sim N(1,4)$
3.For the third situation, we set sample with $X \sim t(1)$,the bimodel distribution is devoted by $(1-\epsilon)N(0,1)+\epsilon N(1,4)$
4.For the fourth situation, we set n1 = 20,n2 = 100.
## Answer 2
```{r}
library(RANN)  #for NN test
library(energy) #for energy test
library(Ball)  #for ball test
library(boot)
Tn <- function(z, ix, sizes,k) {                  
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1) 
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R,
  sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}
m <- 1e3
p.values <- matrix(NA,m,3)




#Unequal variances and equal expectations
k<-3; p<-2; set.seed(12345)
n1 <- n2 <- 20; R<-999; n <- n1+n2; N = c(n1,n2)
eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data=z,statistic=Tn,R=R,
sim = "permutation", sizes = sizes,k=k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1),ncol=p);
y <- matrix(rnorm(n2*p,0,2),ncol=p);
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
alpha <- 0.05                        #confidence level 
pow <- colMeans(p.values<alpha)
print(pow)



```
analysis: from the result(Unequal variances and equal expectations),for the first situation,the ball test get the highest power.

```{r}
#Unequal variances and unequal expectations
eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data=z,statistic=Tn,R=R,
sim = "permutation", sizes = sizes,k=k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1),ncol=p);
y <- matrix(rnorm(n2*p,1,2),ncol=p);
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
alpha <- 0.05                        #confidence level 
pow <- colMeans(p.values<alpha)
print(pow)

```
analysis: from the result,for the second situation(Unequal variances and unequal expectations),the ball test get the highest power.


```{r}
#Non-normal distributions
for(i in 1:m){
x <- matrix(rt(n1*p,df=1),ncol=p);
y <-cbind(rnorm(n2,0,1),rnorm(n2,1,2))
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value#performance of NN method
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value#performance of energy method
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value#performance of ball method
}
alpha <- 0.05                        #confidence level 
pow <- colMeans(p.values<alpha)
print(pow)

```
analysis: from the result,for the third situation(Non-normal distributions),the energy test get the highest power.


```{r}
#Unbalanced samples
n1<-20;n2<-100;N=c(n1,n2)
for(i in 1:m){
  x <- matrix(rt(n1*p,1),ncol=p);
  y <- matrix(rnorm(n2*p,1,2),ncol=p);
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value#performance of NN method
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value#performance of energy method
  p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value#performance of ball method
}
alpha <- 0.05                        #confidence level 
pow <- colMeans(p.values<alpha)
print(pow)

```
analysis: from the result,for the fourth situation(Unbalanced samples),the energy test get the highest power.


#  <font color=Teal>2020/11/17</font>
## Question 9.4
Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.
analysis:
Target distribution: $t_{\nu}$Laplace distribution:$f(x)=\frac{1}{2}e^{-|x|}$
Proposal distribution: $N(X_t,\sigma^2)$
Number of rejection (per 2000)


## Answer 9.4
{\textb}

```{r}
library(knitr)
set.seed(12345)
Ld <-function(x){exp(-abs(x))/2}
rw.Metropolis <- function(sigma, x0, N) {
x <- numeric(N)
x[1] <- x0
        u <- runif(N)
        k <- 0
        for (i in 2:N) {
            y <- rnorm(1, x[i-1], sigma)
                if (u[i] <= (Ld(y)/ Ld(x[i-1])))
                    x[i] <- y  
                else {
                    x[i] <- x[i-1]
                    k <- k + 1
                }
            }
        return(list(x=x, k=k))
    }

    N <- 2000
    sigma <- c(.05, .5,2,16)

    x0 <- 25
    rw1 <- rw.Metropolis(sigma[1], x0, N)
    rw2 <- rw.Metropolis(sigma[2], x0, N)
    rw3 <- rw.Metropolis(sigma[3], x0, N)
    rw4 <- rw.Metropolis(sigma[4], x0, N)
    

#number of candidate points rejected
no.reject <- data.frame(sigma=sigma,no.reject=c(rw1$k, rw2$k, rw3$k, rw4$k))
knitr::kable(no.reject,format='latex',caption = "Number of rejection per 2000")
matrix(c(no.reject[,1],1-no.reject[,2]/2000),ncol = 2,nrow=4,byrow=FALSE,dimnames=list(c(1, 2, 3, 4), c("sigma", "acceptance rates")))
   

   
```


display 4 graphs together with different sigma.
```{r,fig.width=7,fig.height=4}
par(mfrow=c(2,2))  
rw <- cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
for (j in 1:4) {
plot(rw[,j], type="l",
xlab=bquote(sigma == .(round(sigma[j],3))),
ylab="X", ylim=range(rw[,j]))
abline(h=-log(2))
abline(h=log(2))
}
par(mfrow=c(1,1)) #reset to default
```
analysis:among the four chains,in the third plot (σ = 2) the chain is mixing well and converging to the target distribution after a short burn-in period. The fourth chain converges, but it is inefficient.so in the third plot (σ = 2) the chain perform the best.


## Question 2
For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges
approximately to the target distribution according to $\hat R <1.2$.

## Answer 2
```{r}
Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)

psi.means <- rowMeans(psi)     #row means
B <- n * var(psi.means)        #between variance est.
psi.w <- apply(psi, 1, "var")  #within variances
W <- mean(psi.w)               #within est.
v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
r.hat <- v.hat / W             #G-R statistic
return(r.hat)
}

Ld.chain <- function(sigma, N, X1) {
        #generates a Metropolis chain for Normal(0,1)
        #with Normal(X[t], sigma) proposal distribution
        #and starting value X1
        x <- rep(0, N)
        x[1] <- X1
        u <- runif(N)

        for (i in 2:N) {
            xt <- x[i-1]
            y <- rnorm(1, xt, sigma)     #candidate point
            if (u[i] <= (Ld(y)/Ld(xt))) x[i] <- y
            else  x[i] <- xt
            }
        return(x)
}

sigma <- 2     #parameter of proposal distribution
k <- 4          #number of chains to generate
n <- 15000      #length of chains
b <- 500       #burn-in length

#choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)

#generate the chains
set.seed(12345)
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k){
X[i, ] <- Ld.chain(sigma,n,x0[i])
}
#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))

#plot psi for the four chains
# par(mfrow=c(2,2))for (i in 1:k)
 for (i in 1:k)
 if(i==1){
        plot((b+1):n,psi[i, (b+1):n],ylim=c(-0.2,0.2), type="l",
            xlab='Index', ylab=bquote(phi))
      }else{
        lines(psi[i, (b+1):n], col=i)
    }
par(mfrow=c(1,1)) #restore default

```

```{r,echo=FALSE,fig.width=7,fig.height=4}
    
#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in (b+1):n)
rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

```



## Question 11.4
Find the intersection points A(k) in $(0, \sqrt k)$ of the curves  
$S_{k-1}(a)=P(t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}})$  
and  $S_{k}(a)=P(t(k)>\sqrt{\frac{a^2k}{k+1-a^2}})$,
for k = 4 : 25, 100, 500, 1000, where $t(k)$ is a Student t random variable with
k degrees of freedom. (These intersection points determine the critical values
for a t-test for scale-mixture errors proposed by Sz´ekely [260].)

```{r}
b <- c(4:25,100,500,1000)
f <- function(a){
  s1 <- 1-pt((a^2*k/(k+1-a^2))^(1/2),df=k)
  s2 <- 1-pt((a^2*(k-1)/(k-a^2))^(1/2),df=k-1)
  s <- s1-s2
  return(s)
}
e <- 1e-6
root <- numeric(25)
f.root <- numeric(25)
h <- numeric(25)
x <- matrix(0,25,3)
for (i in 1:25) {
  h[i] <- b[i]
  k=h[i]
  root[i] <- uniroot(f,c(0+e,h[i]^(1/2)-e))$root
  f.root[i] < uniroot(f,c(0+e,h[i]^(1/2)-e))$f.root
}
x <- cbind(h,root,f.root)
knitr::kable(x,col.names=c("k","root","f.root"))

```

#  <font color=Teal>2020/11/24</font>
## Question 1
  + A-B-O blood type problem
     + Let the three alleles be A, B, and O.
        
Genotype  | AA|BB |OO |AO |BO |AB |Sum
----------|---|---|---|---|---|---|---
Frequency |p2 |q2 |r2 |2pr|2qr|2pq|1
Count     |nAA|nBB|nOO|nAO|nBO|nAB|n 

 + Observed data: $n_{A\cdot}=n_{AA}+n_{AO}=444$\qquad (A-type), $n_{B\cdot}=n_{BB}+n_{BO}=132$\qquad (B-type), $n_{OO}=361$\qquad (O-type), $n_{AB}=63$\qquad (AB-type).
    
  + Use EM algorithm to solve MLE of $p$\qquad and $q$\qquad (consider missing data $n_{AA}$\qquad and $n_{BB}$\qquad).
    
  + Record the values of p and q that maximize the conditional
likelihood in each EM steps, calculate the corresponding
log-maximum likelihood values (for observed data), are they
increasing?


analysis:
we use $n_A$ to represent $n_{A\cdot}$,$n_B$ to represent $n_{B\cdot}$，and $n_O$ to represent $n_{OO}
1、observed data likelihood:
$$L(p,q|n_A,n_B,n_O,n_{AB})=(p^2+2pr)^{n_A}(q^2+2qr)^{n_B}(r^2)^{n_O}(2pq)^{n_{AB}}$$
where, r=1-p-q
then,find the optimal solution $p_0$,$q_0$  by taking the derivative 
$$L(p|n_A,n_B,n_O,n_{AB})=(p^2+2pr)^{n_A}(q^2+2qr)^{n_B}(r^2)^{n_O}(2pq)^{n_{AB}}$$

2、complete data likelihood
$$L(p,q|n_{AA},n_{AO},n_{BB},n_{BO},n_O,n_{AB})=(p^2)^{n_{AA}}(2pr)^{n_{AO}}(q^2)^{n_{BB}}(2qr)^{n_{BO}}(r^2)^{n_O}(2pq)^{n_{AB}}$$  
$$l(p|n_{AA},n_A,n_{BB},n_B,n_O,n_{AB})=n_{AA}log(\frac{p}{2r})+n_Alog(2pr)+n_{BB}log(\frac{q}{2r})+n_Blog(2qr)+2n_Olog(r)+n_{AB}log(2pq)$$ 
where,$r=1-p-q$



3、E-step
$$E_{\hat p_0,\hat q_0}[l(p,q|n_{AA},n_A,n_{BB},n_B,n_O,n_{AB})]=\frac{p}{p+2r}n_Alog(\frac{p}{2r})+n_Alog(2pr)+\frac{q}{q+2r}n_Blog(\frac{q}{2r})+n_Blog(2qr)+2n_Olog(r)+n_{AB}log(2pq)$$
$n_{AA}|n_A,n_B,n_O,n_{AB} \sim B(n_{A},\frac{p}{p+2r})$
$n_{BB}|n_B,n_A,n_O,n_{AB} \sim B(n_{B},\frac{q}{q+2r})$

4、iterate p,q until they converge to fixed values.


## Answer 1
\textb{}解：

```{r}
library("knitr")
nA <- 444
nB <- 132
nO <- 361
nAB <- 63
n <- sum(c(nA,nB,nO,nAB))
pqr <- matrix(0,100,3)
pql <- matrix(0,10,3)
ll <- numeric(100)

#estimate initial p0,q0,r0
l <- function(x){
  p <- x[1]
  q <- x[2]
  r <- 1-p-q
  -(nA*log(p^2 + 2*p*r)+nB*log(q^2 + 2*q*r) + 2*nO*log(r)+ nAB*log(2*p*q))
}
pq <- optim(c(0.3,0.1),l,method = "CG")$par
p0 <- pq[1]
q0 <- pq[2]
r0 <- 1-p0-q0
pqr[1,] <- c(p0,q0,r0)
ll[1] <- -l(pq)

#the conditional likelihood
El <- function(x,m){
  p <- x[1]
  q <- x[2]
  r <- 1-p-q
  base <- c(p^2,2*p*r,q^2,2*q*r,r^2,2*p*q)
  return(-sum(log(base)*m))
      
}

#EM estimatimate
i <- 1
while((i == 1)||(abs(ll[i]-ll[i-1]) >.Machine$double.eps^0.5)){
p <- pqr[i,1]
q <- pqr[i,2]
r <- 1-p-q
nAA <- nA * p/(p+2*r)
nAO <- nA - nAA
nBB <- nB * q/(q+2*r)
nBO <- nB - nBB
m <- c(nAA,nAO,nBB,nBO,nO,nAB)
res <- optim(c(p,q),El,method="CG",m=m) 
i <- i+1
pqr[i,] <- c(res$par,1-res$par[1]-res$par[2])
ll[i] <- -l(pqr[i,1:2])   #the corresponding log-maximum likelihood values (for observed data)
}

#create table and graph
pql <- cbind(pqr[1:i,1:2],ll[1:i])  #i is the iteration times
round(pql,6)
knitr::kable(pql,col.names = c("p","q","loglikValue"),align = "c",caption="EM estimates")
plot(pql[,3])
   
```
analysis:base on machine percision, (p,q) converge into (0.2976412,	0.1027089)


## Question 2
Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

## Answer 2
```{r}
attach(mtcars)
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
lapply(formulas, lm)

for (i in 1:4) {
  lm(formulas[[i]])
}
detach(mtcars)

```




## Question 3
The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.
trials <- replicate(100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
Extra challenge: get rid of the anonymous function by using
[[ directly.

## Answer 3
```{r}
#Use sapply() and an anonymous function
pvalue1 <- numeric(100)
pvalue2 <- numeric(100)
results <- numeric(100)
i <- 1
trials <- replicate(100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
pvalue1 <- sapply(trials, function(x) x$p.value)

#Extra challenge
results <- replicate(100,expr = {
pvalue2 <- t.test(rpois(10, 10), rpois(7, 10))$p.value
},simplify = FALSE)
pvalue2 <- unlist(results)
```


## Question 4
Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?


## Answer 4
```{r}
set.seed(1)
library(parallel)

Mapply <- function(X, FUN, FUN.VALUE, simplify = FALSE){
  out <- Map(function(x) vapply(x, FUN, FUN.VALUE), X)
  if(simplify == TRUE){return(simplify2array(out))}
  out
}

#example
data1 <- matrix(rnorm(16, 0, 1), nrow = 4)
data2 <- matrix(rnorm(16, 0, 4), nrow = 4)
data <- cbind(data1,data2)
Mapply(data,mean,numeric(1))
```

#  <font color=Teal>2020/12/07</font>
## Question 1
Write an Rcpp function for Exercise 9.4 (page 277, Statistical Computing with R).


## Answer 1
\textb{}解：

```{r,fig.width=7,fig.height=4}
library(knitr)
library(Rcpp)
library(StatComp20040)
N = 2000
sigma = c(.05, .5, 2, 16)
x0 = 25
rw1 = rwc(sigma[1],x0,N)
rw2 = rwc(sigma[2],x0,N)
rw3 = rwc(sigma[3],x0,N)
rw4 = rwc(sigma[4],x0,N)
#number of candidate points rejected
Rej = cbind(rw1[[2]], rw2[[2]], rw3[[2]], rw4[[2]])
Acc = round((N-Rej)/N,4)
rownames(Acc) = "Accept rates"
colnames(Acc) = paste("sigma",sigma)
knitr::kable(Acc)
#plot
par(mfrow=c(2,2))  #display 4 graphs together
rw = cbind(rw1[[1]], rw2[[1]], rw3[[1]], rw4[[1]])
    for (j in 1:4) {
        plot(rw[,j], type="l",
             xlab=bquote(sigma == .(round(sigma[j],3))),
             ylab="X", ylim=range(rw[,j]))
    }

```



## Question 2
Compare the corresponding generated random numbers with those by the R function you wrote before using the function “qqplot”.


## Answer 2
```{r,fig.width=7,fig.height=4}
library(StatComp20040)
set.seed(1234)
N = 2000
sigma = c(.05, .5, 2, 16)
x0 = 25
rw1 = rwc(sigma[1],x0,N)
rw2 = rwc(sigma[2],x0,N)
rw3 = rwc(sigma[3],x0,N)
rw4 = rwc(sigma[4],x0,N)

rwr1 = rw.Metropolis(sigma[1],x0,N)
rwr2 = rw.Metropolis(sigma[2],x0,N)
rwr3 = rw.Metropolis(sigma[3],x0,N)
rwr4 = rw.Metropolis(sigma[4],x0,N)

rw = cbind(rw1[[1]], rw2[[1]], rw3[[1]], rw4[[1]])
rwr = cbind(rwr1$x, rwr2$x, rwr3$x,  rwr4$x)
par(mfrow=c(2,2))  #display 4 graphs together
for (j in 1:4) {
qqplot(rw[,j], rwr[,j],main=bquote(sigma == .(round(sigma[j],3))))
    }


```




## Question 3
Compare the computation time of the two functions with the function “microbenchmark”.

## Answer 3
```{r}
library(microbenchmark)
library(StatComp20040)
N = 2000
sigma = c(.05, .5, 2, 16)
x0 = 25
ts <- microbenchmark(rwc1 = rwc(sigma[1],x0,N),rwr1 = rw.Metropolis(sigma[1],x0,N),times = 4)
summary(ts)[,c(1,3,5,6)]


```


## Question 4
Comments your results.

## Answer 4
from the qqplot, we can see that generated random numbers with two different style code have the similar distribution when $\sigma=0.05$,other situation have larger difference;
from the elapsed time,we can find that, operating with cpp code has higher efficiency, and spend less time than original code.

